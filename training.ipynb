{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Using cached scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.28.1-cp37-cp37m-manylinux2010_x86_64.whl (2.8 MB)\n",
      "Processing /home/dymkiewi/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2/termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting numpy<2.0,>=1.16.0\n",
      "  Using cached numpy-1.18.2-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
      "Processing /home/dymkiewi/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6/wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Using cached protobuf-3.11.3-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Processing /home/dymkiewi/.cache/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e/absl_py-0.9.0-py3-none-any.whl\n",
      "Processing /home/dymkiewi/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3/gast-0.2.2-py3-none-any.whl\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Using cached tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Using cached tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting wheel>=0.26; python_version >= \"3\"\n",
      "  Downloading wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.2.0-py3-none-any.whl (63 kB)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-46.1.3-py3-none-any.whl (582 kB)\n",
      "\u001B[K     |████████████████████████████████| 582 kB 352 kB/s eta 0:00:01\n",
      "\u001B[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.13.1-py2.py3-none-any.whl (87 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001B[K     |████████████████████████████████| 58 kB 714 kB/s eta 0:00:011\n",
      "\u001B[?25hCollecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n",
      "Collecting h5py\n",
      "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Using cached rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "\u001B[K     |████████████████████████████████| 58 kB 712 kB/s eta 0:00:011\n",
      "\u001B[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
      "\u001B[K     |████████████████████████████████| 125 kB 1.3 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.4.5.1-py2.py3-none-any.whl (157 kB)\n",
      "\u001B[K     |████████████████████████████████| 157 kB 596 kB/s eta 0:00:01\n",
      "\u001B[?25hCollecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001B[K     |████████████████████████████████| 133 kB 1.3 MB/s eta 0:00:01\n",
      "\u001B[?25hCollecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001B[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.8 which is incompatible.\u001B[0m\n",
      "Installing collected packages: numpy, scipy, six, grpcio, termcolor, wrapt, google-pasta, setuptools, protobuf, keras-preprocessing, astor, absl-py, gast, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, idna, urllib3, certifi, chardet, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, wheel, markdown, tensorboard, tensorflow-estimator, h5py, keras-applications, opt-einsum, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 cachetools-4.1.0 certifi-2020.4.5.1 chardet-3.0.4 gast-0.2.2 google-auth-1.13.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.28.1 h5py-2.10.0 idna-2.9 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.1 numpy-1.18.2 oauthlib-3.1.0 opt-einsum-3.2.0 protobuf-3.11.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scipy-1.4.1 setuptools-46.1.3 six-1.14.0 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0 urllib3-1.25.8 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --ignore-installed tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-0d833d930cd8>:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.test.is_gpu_available('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scans found: 112120 , Total Headers 112120\n"
     ]
    }
   ],
   "source": [
    "all_xray_df = pd.read_csv('./data/Data_Entry_2017.csv')\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                   glob(os.path.join(os.getcwd(), './data/images_all', '*.png'))}\n",
    "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_xray_df['Finding Labels'] = all_xray_df['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\n",
    "from itertools import chain\n",
    "all_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
    "all_labels = [x for x in all_labels if len(x)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for c_label in all_labels:\n",
    "    if len(c_label)>1: # leave out empty labels\n",
    "        all_xray_df[c_label] = all_xray_df['Finding Labels'].map(lambda finding: '1.0' if c_label in finding else '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 100908 validation 11212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(all_xray_df, \n",
    "                                   test_size = 0.1, \n",
    "                                   random_state = 2137,\n",
    "                                   stratify = all_xray_df['Finding Labels'].map(lambda x: x[:4]))\n",
    "print('train', train_df.shape[0], 'validation', valid_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "IMG_SIZE = (256, 256)\n",
    "core_idg = ImageDataGenerator(samplewise_center=True, \n",
    "                              samplewise_std_normalization=True, \n",
    "                              horizontal_flip = True, \n",
    "                              vertical_flip = False, \n",
    "                              height_shift_range= 0.05, \n",
    "                              width_shift_range=0.1, \n",
    "                              rotation_range=5, \n",
    "                              shear_range = 0.1,\n",
    "                              fill_mode = 'reflect',\n",
    "                              zoom_range=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_xray_df['path'] = all_xray_df['Image Index'].map(lambda x: './data/images_all/'+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100908 validated image filenames.\n",
      "Found 11212 validated image filenames.\n",
      "Found 11212 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "columns=['Atelectasis','Cardiomegaly','Consolidation','Edema','Effusion','Emphysema','Fibrosis','Hernia','Infiltration',\n",
    " 'Mass',\n",
    " 'Nodule',\n",
    " 'Pleural_Thickening',\n",
    " 'Pneumonia',\n",
    " 'Pneumothorax']\n",
    "CLASS_MODE = 'raw'\n",
    "Y_COL = columns\n",
    "DIRESTORY = None\n",
    "X_COL = 'path'\n",
    "CLASSES = all_labels\n",
    "\n",
    "train_gen=core_idg.flow_from_dataframe(\n",
    "                        dataframe=train_df,\n",
    "                        directory=DIRESTORY,\n",
    "                        x_col=X_COL,\n",
    "                        y_col=Y_COL,\n",
    "                        batch_size=12,\n",
    "                        color_mode = 'rgb',\n",
    "                        class_mode=CLASS_MODE,\n",
    "                        target_size=IMG_SIZE)\n",
    "\n",
    "valid_gen=core_idg.flow_from_dataframe(\n",
    "                        dataframe=valid_df,\n",
    "                        directory=DIRESTORY,\n",
    "                        x_col=X_COL,\n",
    "                        y_col=Y_COL,\n",
    "                        batch_size=32,\n",
    "                        color_mode = 'rgb',\n",
    "                        class_mode=CLASS_MODE,\n",
    "                        target_size=IMG_SIZE)\n",
    "\n",
    "test_X, test_Y = next(core_idg.flow_from_dataframe(\n",
    "                        dataframe=valid_df,\n",
    "                        directory=DIRESTORY,\n",
    "                        x_col=X_COL,\n",
    "                        y_col=Y_COL,\n",
    "                        batch_size=11212 ,\n",
    "                        color_mode = 'rgb',\n",
    "                        class_mode=CLASS_MODE,\n",
    "                        target_size=IMG_SIZE)) # one big batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tf-nightly==2.2.0-dev20200206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from efficientnet.keras import EfficientNetB1\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "base_model = EfficientNetB1(input_shape =  (256,256,3), \n",
    "                                 include_top = False, weights = 'imagenet')\n",
    "multi_disease_model = Sequential()\n",
    "multi_disease_model.add(base_model)\n",
    "multi_disease_model.add(GlobalAveragePooling2D())\n",
    "multi_disease_model.add(Dense(128))\n",
    "multi_disease_model.add(BatchNormalization())\n",
    "multi_disease_model.add(Activation('relu'))\n",
    "multi_disease_model.add(Dense(128))\n",
    "multi_disease_model.add(Activation('relu'))\n",
    "\n",
    "multi_disease_model.add(Dense(len(all_labels), activation = 'sigmoid'))\n",
    "\n",
    "multi_disease_model.compile(optimizer = 'adam', loss = 'binary_crossentropy'\n",
    "                            ,metrics =[tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_.hdf5\".format('imagenet_from_scratch_with_auc_metrics_multilabel_false')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_auc_1', verbose=1, \n",
    "                             save_best_only=True, mode='auto', save_weights_only = True)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=15)\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_disease_model.load_weights('./data/xray_class_weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_disease_model.load_weights('./xray_class_efficientNet.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_disease_model.load_weights('./imagenet_from_scratch_.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred_Y = multi_disease_model.predict(test_X, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_Y = test_Y.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "avg = []\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
    "for (idx, c_label) in enumerate(all_labels):\n",
    "    fpr, tpr, thresholds = roc_curve(test_Y[:,idx].astype(int), pred_Y[:,idx])\n",
    "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    avg.append(auc(fpr, tpr).astype(np.float))\n",
    "    \n",
    "print(sum(avg) / len(avg))\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "15/15 [==============================] - 12s 816ms/step - loss: 0.1837 - auc_1: 0.5237 - val_loss: 0.2996 - val_auc_1: 0.5246\n",
      "\n",
      "Epoch 00001: val_auc_1 improved from inf to 0.52457, saving model to imagenet_from_scratch_with_auc_metrics_multilabel_false_.hdf5\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 12s 800ms/step - loss: 0.1724 - auc_1: 0.5255 - val_loss: 0.4408 - val_auc_1: 0.5261\n",
      "\n",
      "Epoch 00002: val_auc_1 did not improve from 0.52457\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 12s 796ms/step - loss: 0.1991 - auc_1: 0.5264 - val_loss: 7.1569 - val_auc_1: 0.5265\n",
      "\n",
      "Epoch 00003: val_auc_1 did not improve from 0.52457\n",
      "Epoch 4/5\n",
      "14/15 [===========================>..] - ETA: 0s - loss: 0.1766 - auc_1: 0.5265"
     ]
    }
   ],
   "source": [
    "multi_disease_model.fit_generator(train_gen, \n",
    "                                  steps_per_epoch=15,\n",
    "                                  validation_data = valid_gen, \n",
    "                                  validation_steps = 20,\n",
    "                                  epochs = 5, \n",
    "                                  callbacks = callbacks_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}